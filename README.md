Atmospheric turbulence is a physical phenomenon where changes in air density, temperature, humidity, and many other factors cause large bodies of air to move chaotically. When a scene is observed through this turbulence it appears distorted with a combination of a visual rippling, localized blurring, and variations in brightness. These effects are pronounced when either viewed over long distances, or local variations in temperature are high. Fields such as long range photography, astrophotography, high altitude surveillance or cases where temperature variations are high like desert environments, near asphalt, or exhaust from cars and buildings are particularly impacted by atmospheric turbulence.

In this project, I explore the use of a typical U-net denoising architecture augmented with a recurrent neural network (RNN) block. The addition of an RNN block allows the model to accumulate information from frame by frame and determine which features of the sequence are are atmospheric distortions and which parts are ground truth objects that are moving. Using the features accumulated by the RNN block, a U-net block can then remove noise and distortion while leaving the ground truth features intact. In addition to the RNN block, I restrict the architecture to only use causal information and the model is trained only using the current or past frames and not future frames. I also prioritize inference speed and model simplicity. This results in a real-time, causal, atmospheric denoising algorithm with better deployability in real world applications.

For a complete report of the experiments and results, see Final_Report.pdf. 
